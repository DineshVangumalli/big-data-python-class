{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.1  Register for a stream of Twitter data\n",
    "\n",
    "5.2  Create a bloom filter classifying two days worth of twitters  ( after removing stop words and urls )\n",
    "\n",
    "5.3  For another days worth of twitter data find the previous twitters that match in the bloom filter\n",
    "(This means get two days of data in one file or directory , use that data for training the bloom filter, capture a different days data in a different file ( or do it in real time)and capture the match output then running the new twitter data through the filter.\n",
    "\n",
    "5.4 Plot a historgram of matches for each twitter in 5.3\n",
    "\n",
    "For the 4-5 grade.- Submit in a separate notebook - YourNAME-Homework5-Supplement\n",
    "\n",
    "1. Use a different machine learning training algorithm\n",
    "2. Make a continous feed where you take two days of data and match the incoming stream ( do this for 5 days windowing the filter data)\n",
    "3. Find new trends in the twitter feed (daily or hourly)\n",
    "4. Or some other streaming exploration of your choosing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting code/twitter_streaming_sports_DV.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile code/twitter_streaming_sports_DV.py\n",
    "# %load code/twitter_streaming_sports_DV.py\n",
    "\n",
    "\n",
    "#Import the necessary methods from tweepy library\n",
    "from tweepy.streaming import StreamListener\n",
    "import tweepy\n",
    "from tweepy import OAuthHandler\n",
    "from tweepy import Stream\n",
    "\n",
    "#Variables that contains the user credentials to access Twitter API\n",
    "access_token = \"131711504-uteEX6y8zoi0v4miLzEjZefg22ZZ8TgukyU9bHYW\"\n",
    "access_token_secret = \"UH3dk5s00euTgzJ92xveSSi7b84Aw2rrrtFeHelV5rmIv\"\n",
    "consumer_key = \"7lKe94q4mhbeINgp1hFWmpdGj\"\n",
    "consumer_secret = \"T46JL4SAhMK3EmiXpCFzSVwQ6ipvQ0xfqHBGbAUbPBVjtbDDp1\"\n",
    "\n",
    "\n",
    "#This is a basic listener that just prints received tweets to stdout.\n",
    "class StdOutListener(StreamListener):        \n",
    "    def on_data(self, data):\n",
    "        print(data)\n",
    "        return True\n",
    "\n",
    "    def on_error(self, status):\n",
    "        print(status)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    #This handles Twitter authetification and the connection to Twitter Streaming API\n",
    "    l = StdOutListener()\n",
    "    auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "    auth.set_access_token(access_token, access_token_secret)\n",
    "    stream = Stream(auth, l)\n",
    "\n",
    "    #This line filter Twitter Streams to capture data by the keywords: 'cricket', 'soccer', 'basketball', 'baseball', 'tennis'\n",
    "    stream.filter(track=['cricket', 'soccer', 'basketball', 'baseball', 'tennis'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%sh  \n",
    "python code/Twitter_streaming_sports_DV.py > tweepy_sports_test1.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean the train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing code/twitter_clean_train_DV.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile code/twitter_clean_train_DV.py\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\D+')\n",
    "\n",
    "with open('data/tweepy_sports_train.txt') as f:\n",
    "    for word in f:\n",
    "        if word.strip()!='' and len(word.split())>=3:\n",
    "            word_list=tokenizer.tokenize(word.lower().replace('\"text\":','').replace('\\\\u','').replace('https','').replace('\\\\','').replace('//','')\\\n",
    "                                         .replace('/','').replace('\\\\','').replace('\"','').split(',')[3])\n",
    "           \n",
    "            for word_ in word_list:\n",
    "                if word_ not in stopwords.words('english') and not(word_.startswith('u30')) and len(word_.strip())>2:\n",
    "                    print(word_.split())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bash.exe: warning: could not find /tmp, please create!\n"
     ]
    }
   ],
   "source": [
    "%%sh  \n",
    "python code/twitter_clean_train_DV.py > data/tweepy_cleaned_train.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting code/twitter_clean_test_DV.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile code/twitter_clean_test_DV.py\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\D+')\n",
    "\n",
    "with open('data/tweepy_sports_test.txt') as f:\n",
    "    for word in f:\n",
    "        if word.strip()!='' and len(word.split())>=3:\n",
    "            word_list=tokenizer.tokenize(word.lower().replace('\"text\":','').replace('\\\\u','').replace('https','').replace('\\\\','').replace('//','')\\\n",
    "                                         .replace('/','').replace('\\\\','').replace('\"','').split(',')[3])\n",
    "           \n",
    "            for word_ in word_list:\n",
    "                if word_ not in stopwords.words('english') and not(word_.startswith('u30')) and len(word_.strip())>2:\n",
    "                    print(word_.split())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bash.exe: warning: could not find /tmp, please create!\n"
     ]
    }
   ],
   "source": [
    "%%sh  \n",
    "python code/twitter_clean_test_DV.py > data/tweepy_cleaned_test.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(cricket,216)\n",
      "(soccer,613)\n",
      "(basketball,226)\n",
      "(baseball,104)\n",
      "(tennis,147)\n",
      "\n",
      "\n",
      "################### TOP 10 WORDS ######################\n",
      "(rt,16781)\n",
      "( the,10354)\n",
      "( to,8050)\n",
      "( a,6313)\n",
      "( soccer,5337)\n",
      "( of,5289)\n",
      "( basketball,5178)\n",
      "( in,5154)\n",
      "( is,4319)\n",
      "( and,4025)\n"
     ]
    }
   ],
   "source": [
    "from pybloom import ScalableBloomFilter\n",
    "sbf = ScalableBloomFilter(mode=ScalableBloomFilter.SMALL_SET_GROWTH)\n",
    "\n",
    "final_dict={}\n",
    "with open('data/tweepy_cleaned_train.txt') as file:\n",
    "    for word in file:\n",
    "        word_list=word.replace('[','').replace(']','').replace(\"'\",'').split(',')\n",
    "        for tweet_word in word_list:\n",
    "            sbf.add(tweet_word)\n",
    "\n",
    "with open('data/tweepy_cleaned_test.txt') as file:\n",
    "    for word in file:\n",
    "        word_list=word.replace('[','').replace(']','').replace(\"'\",'').split(',')\n",
    "        for tweet_word in word_list:\n",
    "            if tweet_word in sbf:\n",
    "                final_dict[tweet_word]=final_dict.get(tweet_word,0) + 1\n",
    "\n",
    "\n",
    "print(\"(%s,%s)\"%(\"cricket\",final_dict['cricket']))\n",
    "print(\"(%s,%s)\"%(\"soccer\",final_dict['soccer']))\n",
    "print(\"(%s,%s)\"%(\"basketball\",final_dict['basketball']))\n",
    "print(\"(%s,%s)\"%(\"baseball\",final_dict['baseball']))\n",
    "print(\"(%s,%s)\"%(\"tennis\",final_dict['tennis']))\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "counter=0\n",
    "print(\"################### TOP 10 WORDS ######################\")\n",
    "for key,value in sorted(final_dict.iteritems(),key=lambda (k,v):(v,k),reverse=True):\n",
    "    counter+=1\n",
    "    print(\"(%s,%s)\"%(key,value))\n",
    "    if counter>=10: break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tried using dynamic_pybloom (which worked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(cricket,216)\n",
      "(soccer,613)\n",
      "(basketball,226)\n",
      "(baseball,104)\n",
      "(tennis,147)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from dynamic_pybloom import ScalableBloomFilter\n",
    "sbf = ScalableBloomFilter(mode=ScalableBloomFilter.SMALL_SET_GROWTH)\n",
    "\n",
    "final_dict={}\n",
    "with open('data/tweepy_cleaned_train.txt') as file:\n",
    "    for word in file:\n",
    "        word_list=word.replace('[','').replace(']','').replace(\"'\",'').split(',')\n",
    "        for tweet_word in word_list:\n",
    "            sbf.add(tweet_word)\n",
    "\n",
    "with open('data/tweepy_cleaned_test.txt') as file:\n",
    "    for word in file:\n",
    "        word_list=word.replace('[','').replace(']','').replace(\"'\",'').split(',')\n",
    "        for tweet_word in word_list:\n",
    "            if tweet_word in sbf:\n",
    "                final_dict[tweet_word]=final_dict.get(tweet_word,0) + 1\n",
    "\n",
    "\n",
    "print(\"(%s,%s)\"%(\"cricket\",final_dict['cricket']))\n",
    "print(\"(%s,%s)\"%(\"soccer\",final_dict['soccer']))\n",
    "print(\"(%s,%s)\"%(\"basketball\",final_dict['basketball']))\n",
    "print(\"(%s,%s)\"%(\"baseball\",final_dict['baseball']))\n",
    "print(\"(%s,%s)\"%(\"tennis\",final_dict['tennis']))\n",
    "\n",
    "print('\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
